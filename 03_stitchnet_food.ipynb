{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02463b7-9fb2-4fc9-b3d4-963da1093a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46146e10-04f2-4fa4-a001-24fdf1398844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skl2onnx.helpers.onnx_helper import load_onnx_model\n",
    "from stitchnet.stitchonnx.utils import Net\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "netsFiles = sorted(glob('_models/fragments/net*'))\n",
    "nets = []\n",
    "for i,netsFile in enumerate(netsFiles):\n",
    "    fragmentFiles = sorted(glob(str(Path(netsFile)/'fragment*.onnx')))\n",
    "    onnxFragments = []\n",
    "    for fragmentFile in fragmentFiles:\n",
    "        onnxFragment = load_onnx_model(fragmentFile)\n",
    "        onnxFragments.append(onnxFragment)\n",
    "    net1 = Net(onnxFragments, i)\n",
    "    nets.append(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9078a9b6-bf3b-41ae-aa3e-09fed02a84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'best' in 'best1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd9e874-d12e-4744-8c34-f3ca6aa5e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.ones(1,2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "991aecb8-aee4-4bc0-a464-0b308014566d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stitchnet.stitchonnx.utils import load_cats_and_dogs_dset,convert_imagenet_to_cat_dog_label\n",
    "from stitchnet.stitchonnx.utils import accuracy_score_model,accuracy_score_net,load_dl\n",
    "from stitchnet.stitchonnx.utils import generate_networks, ScoreMapper\n",
    "from stitchnet.stitchonnx.report import Report\n",
    "from stitchnet.stitchonnx.utils import evalulate_stitchnet\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(51)\n",
    "np.random.seed(24)\n",
    "torch.manual_seed(77)\n",
    "\n",
    "K = 2\n",
    "STITCH_BATCH_SIZE = 96 # todo study the effect\n",
    "MAX_DEPTH = 10\n",
    "THRESOULD = 0.8\n",
    "TOTAL_THRESOULD = 0.8\n",
    "\n",
    "RESULT_NAME = f\"{int(time.time())}_result_food101_CKA_BS_{STITCH_BATCH_SIZE}_MD_{MAX_DEPTH}_T_{THRESOULD}_TT_{TOTAL_THRESOULD}_K_{K}\"\n",
    "\n",
    "EVAL_BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# data_score = data_score.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f901b9-ed7e-4e37-88c6-77b978c454e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stitchnet.stitchonnx.utils import load_hf_train_val_dset_with_test_split\n",
    "\n",
    "# dataset_train, dataset_val = load_hf_train_val_dset_with_test_split('food101',train='train', val='validation',label=\"label\",num_train=10000, num_val=1000)\n",
    "\n",
    "# dataset_train.save_to_disk('./_data/food101/train')\n",
    "# dataset_val.save_to_disk('./_data/food101/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f946f-1e05-4eef-85b8-030204301cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7794a6d6-18b0-47c5-9d12-13d92fc0dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8513d1e6-42c1-4545-a3fa-03ce31119c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_from_disk('./_data/food101/train', keep_in_memory=True)\n",
    "dataset_val = load_from_disk('./_data/food101/validation', keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0a03f3-c7dd-49f4-9cea-c89f1a4e03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train.features['label'].int2str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "824fc872-d4bf-42d5-82e8-c1fa9e029f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_labels = [0,1,2,3,4]\n",
    "dataset_train = dataset_train.filter(lambda example: example['label'] in selected_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ef4ca7-50dd-4ba8-88c5-42c46c048a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5167fc18-6864-4fbf-9366-995f0ac2f489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_val = dataset_val.filter(lambda example: example['label'] in selected_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be905c2c-d552-4e18-b5fc-412e345c1344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label', 'pixel_values'],\n",
       "    num_rows: 483\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c040166d-8cc6-4fe6-ae88-675d4637e99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label', 'pixel_values'],\n",
       "    num_rows: 49\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08c70e-93ac-42cf-9787-0ac02b01095e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fd8a2cb-60b6-4a66-b77b-38b4ee395289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = dataset_train.with_format(type='torch')\n",
    "# dataset_val = dataset_val.with_format(type='torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7849cd6a-df93-4501-aab3-18b1492b11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_to_disk('./_data/food101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad850b19-f2d6-4be7-90b3-e706e416624d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def collate(items):\n",
    "#     inps = []\n",
    "#     tgts = []\n",
    "#     for item in items:\n",
    "#         inps.append(item['pixel_values'])\n",
    "#         tgts.append(item['label'])\n",
    "#     return torch.stack(inps,0), torch.stack(tgts,0)\n",
    "    \n",
    "# # dl_score = load_dl(dataset_train, batch_size=STITCH_BATCH_SIZE, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d737401e-3326-4ee9-8824-353373263b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STITCH_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26daa19a-7f9a-40dd-adcc-82bfa34bd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.stack(x['pixel_values']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9a1ea04-be59-41e9-9d7e-31a10f7ec9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3417ae7d-11e6-4e13-9155-98fabcd8bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.vstack(dataset_train['pixel_values']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b00f461-0809-4733-83f4-d250d75befaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:13<00:00,  7.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(96, 3, 224, 224)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoProcessor\n",
    "from tqdm import tqdm\n",
    "# p = AutoProcessor.from_pretrained('microsoft/resnet-50')\n",
    "inps = []\n",
    "tgts = []\n",
    "# for x in tqdm(dataset_train.take(STITCH_BATCH_SIZE)):\n",
    "for x in tqdm(dataset_train.select(range(STITCH_BATCH_SIZE))):\n",
    "    try:\n",
    "        inps += x['pixel_values']\n",
    "        # inps += p(x['image'])['pixel_values']\n",
    "        tgts += [x['label']]\n",
    "    except:\n",
    "        pass\n",
    "data_score = np.stack(inps)\n",
    "data_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62974d6c-c3d1-4a1b-bd2d-b072a78680f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset('food101', streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6e38f4-1cc8-42d1-8ec9-9911419e9f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset['validation']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a977b3aa-cda4-43ef-b570-91c089254689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03b7c4c8-4fa9-497c-9533-22407873e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoProcessor\n",
    "# p = AutoProcessor.from_pretrained('microsoft/resnet-50')\n",
    "# def processor_function(examples):\n",
    "#     try:\n",
    "#         r = p(examples[\"image\"])\n",
    "#         return r\n",
    "#     except:\n",
    "#         return {'pixel_values': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7e6d8ff-1923-4490-8fcf-b8021f74673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(processor_function, \n",
    "#             batched=False,writer_batch_size=30000,keep_in_memory=False).filter(lambda x:x['pixel_values'] is not None, \n",
    "#                batched=False,writer_batch_size=30000,keep_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4db195c0-96cf-40a6-94c0-8a08a640fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.remove_columns('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24ff8288-3568-45fe-abd7-64581e28dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_to_disk('./_data/food101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4cde6ee-98b6-4e93-9fb1-f2b6890d1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d96d197-861d-4ef7-b9e0-be47ad6d5fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb43cff6-4cc6-4b80-8302-e771fa351582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc176f7d-eb58-45a6-bd5c-a41f29e9aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(task_evaluator.compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da3354-33bd-4712-9355-48b95ec0834c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a7432e2-0014-43f5-a125-2da0b49ccc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = dataset[\"validation\"].select([1,2,3]).map(processor_function, \n",
    "#             batched=False,writer_batch_size=30000,keep_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68233df8-f232-4eb2-93fb-8ae71b769b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.remove_columns(['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed3e7f47-3f89-4ef9-a50a-4bf06252e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = dataset[\"validation\"].select([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76f6f105-1124-411c-a070-d56a0cca8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(ds[0]['pixel_values']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559fd5e-ac6d-4ac0-8e32-6edecf700332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3304b149-1b7c-46ec-80c6-9df32dd23f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.array(ds['pixel_values'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5e233d8-bb8e-439b-a105-9161cd69aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42fe002a-c930-4e3b-a6f5-997426e6b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f061c2b7-40ba-4a10-aaae-8193e0ca6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "524129d5-f041-47e5-82f4-4c04b43f6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Image.open(\"./PNG_Test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19c246af-db2e-415c-8ed3-bd82c7ca0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stitchnet.stitchonnx.utils import accuracy_score_model,accuracy_score_net,load_dl\n",
    "# ds = load_dl([[x],[x],[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c86d4ae-c957-4f73-bbb1-ab0dfcd9b97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abbcb198-5c11-40c6-8b24-de5369a73b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from evaluate import evaluator\n",
    "# task_evaluator = evaluator(\"image-classification\")\n",
    "\n",
    "# class EvalPipeline:\n",
    "#     def __init__(self, net):\n",
    "#         self.task = \"image-classification\"\n",
    "#         self.net = net\n",
    "#     def __call__(self, inps, **kwargs):\n",
    "#         # inps\n",
    "#         # return []\n",
    "#         # inps = [1]\n",
    "#         print('inps', len(inps), inps[0])\n",
    "#         result = [[{\n",
    "#             \"score\": 0.1,\n",
    "#             \"label\": 6\n",
    "#         }] for p in inps]\n",
    "#         # print('result', result)\n",
    "#         return result\n",
    "# net = None\n",
    "# pipe = EvalPipeline(net)\n",
    "# ds = dataset['validation']\n",
    "# # label_mapping = {}\n",
    "# # for name in ds.features['label'].names:\n",
    "# #     label_mapping[name] = ds.features['label'].str2int(name)\n",
    "# task_evaluator.compute(pipe, \n",
    "#                        data=ds,\n",
    "#                        metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "658174d4-4fbb-47f7-bb8f-af77ae963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d89a93d9-02bd-4147-b645-14cce46ac44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfc2c932-6ed2-4b27-9c2b-7245487f2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([{\"score\": 1}, {\"score\": 2}], key=lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8912ac3-0aed-41a4-a513-67ac966a4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4279de36-b293-4a6a-9677-802c3cea8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['train'].features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "caa4918f-62af-44a7-9da7-11c6ed54627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['train'].features['label'].int2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31ca1aca-33d0-4314-ac26-33881600e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(dataset['train'].features['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ddd934b1-39e2-4e7f-89da-f0032120b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_val.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "492c1c0e-2552-4c03-b035-2f463814e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(d['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0ffa569-3883-43eb-abd6-d3f408c65bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1154196-1305-4c84-8f70-6ad82312e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_score, t = next(iter(dl_score))\n",
    "# print(data_score,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "371eba24-9ff0-4135-9636-2cee667db4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1cedaed-9513-4d35-a689-0d4f061ab4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6b5fab6-3aff-4191-ac4a-01692b20cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [None] is not [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5bf88f6-d705-4f1d-a1a1-4bf1f1dfa141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7eb4d2c8-d9e7-40b2-bd4e-4580da3247f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d30b172-977a-4a3a-9cf7-999c9b42abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train[0]['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdd08d39-bb7e-43d3-85e4-b0d03962edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# arr = [t for (x,t) in dataset_train]\n",
    "# # Get unique values and their counts\n",
    "# unique_values, counts = np.unique(arr, return_counts=True)\n",
    "# # Print the results\n",
    "# for value, count in zip(unique_values, counts):\n",
    "#     print(f\"TRAIN: {value} occurs {count} times.\")\n",
    "    \n",
    "# arr = [t for (x,t) in dataset_val]\n",
    "# # Get unique values and their counts\n",
    "# unique_values, counts = np.unique(arr, return_counts=True)\n",
    "# # Print the results\n",
    "# for value, count in zip(unique_values, counts):\n",
    "#     print(f\"VAL: {value} occurs {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecbec2f6-cd74-4507-aef1-2329b9a6247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb5e71f2-1d9c-416f-b0ab-b90f1ebc76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range(1)\n",
    "k = 0\n",
    "if os.path.exists(f'./_results/{RESULT_NAME}.txt'):\n",
    "    with open(f'./_results/{RESULT_NAME}.txt', 'r') as f:\n",
    "        k = len(f.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e0f7bd-afd3-48af-9574-2f757e54333b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac786595-9118-4911-8f61-770ab970791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataItem in tqdm(dataset_train):\n",
    "#     print(dataItem['pixel_values'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ef687c5-6b30-4c10-8536-e0eb218da7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 3, 224, 224)\n",
      "(96, 64, 27, 27)\n",
      "(96, 192, 13, 13)\n",
      "(96, 384, 13, 13)\n",
      "(96, 256, 13, 13)\n",
      "(96, 9216)\n",
      "(96, 4096)\n",
      "(96, 4096)\n",
      "current depth: 1\n",
      "(96, 3, 224, 224)\n",
      "(96, 3, 224, 224)\n",
      "(96, 3, 224, 224)\n",
      "(96, 16, 56, 56)(96, 3, 224, 224)\n",
      "\n",
      "(96, 64, 56, 56)(96, 16, 56, 56)\n",
      "\n",
      "(96, 72, 28, 28)(96, 64, 224, 224)(96, 256, 56, 56)(96, 256, 56, 56)\n",
      "\n",
      "\n",
      "\n",
      "(96, 64, 112, 112)\n",
      "(96, 24, 28, 28)\n",
      "(96, 96, 14, 14)(96, 512, 28, 28)(96, 128, 112, 112)\n",
      "(96, 512, 28, 28)\n",
      "\n",
      "\n",
      "(96, 128, 56, 56)\n",
      "(96, 40, 14, 14)\n",
      "(96, 256, 56, 56)\n",
      "(96, 1024, 14, 14)\n",
      "(96, 120, 14, 14)\n",
      "(96, 1024, 14, 14)\n",
      "(96, 256, 56, 56)(96, 2048)\n",
      "\n",
      "(96, 48, 14, 14)(96, 256, 28, 28)\n",
      "\n",
      "(96, 512, 28, 28)\n",
      "(96, 1024)\n",
      "(96, 288, 7, 7)\n",
      "(96, 512, 28, 28)(96, 96, 7, 7)\n",
      "\n",
      "(96, 576)\n",
      "(96, 1024)(96, 512, 14, 14)\n",
      "\n",
      "(96, 512, 14, 14)\n",
      "(96, 512, 14, 14)\n",
      "(96, 25088)\n",
      "(96, 4096)\n",
      "(96, 4096)\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff42556e90>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7eff42556530>\n",
      "totalscore 1.0\n",
      "diff sampled tensor(0.)\n",
      "epoch 0 loss 3.882599307343567e-11 torch.Size([69984, 64]) torch.Size([69984, 64])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 2\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5710b880>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb11360>\n",
      "totalscore 1.0\n",
      "diff sampled tensor(1.2443e-05)\n",
      "epoch 0 loss 1.955038871095593e-09 torch.Size([16224, 192]) torch.Size([16224, 192])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 3\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff42557340>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72a10>\n",
      "totalscore 0.9999998807907104\n",
      "diff sampled tensor(2.7280e-05)\n",
      "epoch 0 loss 3.180483435678359e-09 torch.Size([16224, 384]) torch.Size([16224, 384])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 4\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe77ee64a0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70bb0>\n",
      "totalscore 0.9999998807907104\n",
      "diff sampled tensor(2.2330e-05)\n",
      "epoch 0 loss 1.9156251636455275e-09 torch.Size([16224, 256]) torch.Size([16224, 256])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 5\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.89']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff42554b50>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70af0>\n",
      "totalscore 0.9999999999999858\n",
      "epoch 0 loss 0.0 torch.Size([96, 9216]) torch.Size([96, 9216])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 6\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.95']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5706f820>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70a90>\n",
      "totalscore 0.9999998807906962\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 7\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.95']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5706efe0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb700d0>\n",
      "totalscore 0.9999998807906962\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 71.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.4897959183673469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net000\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5706efe0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70a90>\n",
      "totalscore 0.946288532461076\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 8\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.95']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe57108eb0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb700d0>\n",
      "totalscore 0.9462884760578842\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 52.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.5102040816326531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net001\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe57108eb0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70a90>\n",
      "totalscore 0.8954641239296556\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 9\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c5777c0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb700d0>\n",
      "totalscore 0.8954641239296556\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 74.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.5510204081632653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net002\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5706f820>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb700d0>\n",
      "totalscore 0.9462887644767627\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 76.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.5102040816326531\n",
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff42554b50>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71d20>\n",
      "totalscore 0.89446092910039\n",
      "epoch 0 loss 0.0 torch.Size([96, 9216]) torch.Size([96, 25088])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 6\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.94']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c5145e0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71c60>\n",
      "totalscore 0.89446092910039\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 7\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.94']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe43217e50>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70340>\n",
      "totalscore 0.894460822472338\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 85.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.4489795918367347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net004\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe43217e50>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71c60>\n",
      "totalscore 0.8427958526643327\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 8\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.94']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe43217850>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70340>\n",
      "totalscore 0.8427958526643327\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 75.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.42857142857142855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net005\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe43217850>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71c60>\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c5145e0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70340>\n",
      "totalscore 0.8427938267313467\n",
      "epoch 0 loss 0.0 torch.Size([96, 4096]) torch.Size([96, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 82.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.5510204081632653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net006\n",
      "current depth: 1\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71fc0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71630>\n",
      "totalscore 1.0\n",
      "diff sampled tensor(0.)\n",
      "epoch 0 loss 8.09507258806718e-09 torch.Size([301056, 256]) torch.Size([301056, 256])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 2\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe37ed64a0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71720>\n",
      "totalscore 1.0\n",
      "diff sampled tensor(0.0008)\n",
      "epoch 0 loss 1.051524202254001e-08 torch.Size([75264, 512]) torch.Size([75264, 512])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 3\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c0ebaf0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71210>\n",
      "totalscore 1.0\n",
      "diff sampled tensor(0.0018)\n",
      "epoch 0 loss 7.558488895093823e-08 torch.Size([18816, 1024]) torch.Size([18816, 1024])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 4\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.83']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c517f10>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb711b0>\n",
      "totalscore 1.0\n",
      "epoch 0 loss 0.0 torch.Size([96, 1024]) torch.Size([96, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 13.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n",
      "(17, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6530612244897959\n",
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net007\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c517f10>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72770>\n",
      "totalscore 0.8265362977981567\n",
      "epoch 0 loss 0.0 torch.Size([96, 1024]) torch.Size([96, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n",
      "(17, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6938775510204082\n",
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net008\n",
      "current depth: 1\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.95']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72920>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72bf0>\n",
      "totalscore 1.0\n",
      "diff sampled tensor(0.)\n",
      "epoch 0 loss 1.0865470477998322e-08 torch.Size([301056, 16]) torch.Size([301056, 16])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 2\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.86']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe571ca530>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72ce0>\n",
      "totalscore 1.0\n",
      "diff sampled tensor(0.0466)\n",
      "epoch 0 loss 2.7762284500021762e-08 torch.Size([301056, 16]) torch.Size([301056, 16])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 3\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff4253bb50>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72260>\n",
      "totalscore 1.0\n",
      "diff sampled tensor(0.0008)\n",
      "epoch 0 loss 5.952946858056736e-09 torch.Size([75264, 72]) torch.Size([75264, 72])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 4\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe37f13190>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb73130>\n",
      "totalscore 1.0000001192092896\n",
      "diff sampled tensor(0.0027)\n",
      "epoch 0 loss 3.31601235601039e-08 torch.Size([75264, 24]) torch.Size([75264, 24])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 5\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c0ebfd0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71e70>\n",
      "totalscore 1.0000001192092896\n",
      "diff sampled tensor(0.0002)\n",
      "epoch 0 loss 8.982963114097705e-09 torch.Size([18816, 96]) torch.Size([18816, 96])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 6\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe37f132e0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71810>\n",
      "totalscore 1.0000001192092896\n",
      "diff sampled tensor(0.1329)\n",
      "epoch 0 loss 4.668877761792012e-06 torch.Size([18816, 40]) torch.Size([18816, 40])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 7\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5706f700>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71330>\n",
      "totalscore 1.0000001192092896\n",
      "diff sampled tensor(0.0636)\n",
      "epoch 0 loss 2.096683057389368e-06 torch.Size([18816, 120]) torch.Size([18816, 120])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 8\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe57109ab0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70a30>\n",
      "totalscore 1.0000001192092896\n",
      "diff sampled tensor(0.0985)\n",
      "epoch 0 loss 3.6394544777129366e-06 torch.Size([18816, 48]) torch.Size([18816, 48])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 9\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 0 []\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe571ca530>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72bf0>\n",
      "totalscore 0.8628056049346924\n",
      "diff sampled tensor(8781044.)\n",
      "epoch 0 loss 3.959399786125235 torch.Size([301056, 16]) torch.Size([301056, 16])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 3\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 0 []\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72920>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72ce0>\n",
      "totalscore 0.9452014565467834\n",
      "diff sampled tensor(8780426.)\n",
      "epoch 0 loss 0.7049192034062886 torch.Size([301056, 16]) torch.Size([301056, 16])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 2\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['0.98']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c517f10>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72260>\n",
      "totalscore 0.9226701540879105\n",
      "diff sampled tensor(62512.4922)\n",
      "epoch 0 loss 0.5376060669924937 torch.Size([75264, 72]) torch.Size([75264, 72])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 3\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['0.97']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c0eb940>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb73130>\n",
      "totalscore 0.8926791680298134\n",
      "diff sampled tensor(202502.1875)\n",
      "epoch 0 loss 2.433029502427497 torch.Size([75264, 24]) torch.Size([75264, 24])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 4\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['0.98']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff425553c0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71e70>\n",
      "totalscore 0.8762941376576842\n",
      "diff sampled tensor(10907.0762)\n",
      "epoch 0 loss 0.43603026948007595 torch.Size([18816, 96]) torch.Size([18816, 96])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 5\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['0.97']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff42554310>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71810>\n",
      "totalscore 0.8470586631749445\n",
      "diff sampled tensor(89053.3594)\n",
      "epoch 0 loss 4.422189517897003 torch.Size([18816, 40]) torch.Size([18816, 40])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 6\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['0.97']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe571f1510>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71330>\n",
      "totalscore 0.8182252120329163\n",
      "diff sampled tensor(30429.5352)\n",
      "epoch 0 loss 1.5286511661244087 torch.Size([18816, 120]) torch.Size([18816, 120])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 7\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['0.97']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe37f13160>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70a30>\n",
      "current depth: 1\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70880>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71db0>\n",
      "totalscore 0.9999998807907104\n",
      "diff sampled tensor(0.)\n",
      "epoch 0 loss 1.9202192779154795e-08 torch.Size([301056, 64]) torch.Size([301056, 64])\n",
      "diff sampled tensor(0.)\n",
      "epoch 0 loss 1.93812622609976e-08 torch.Size([301056, 64]) torch.Size([301056, 64])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 2\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe57108dc0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71f90>\n",
      "totalscore 0.9999997615814351\n",
      "diff sampled tensor(0.0149)\n",
      "epoch 0 loss 3.534992282567953e-08 torch.Size([301056, 256]) torch.Size([301056, 256])\n",
      "diff sampled tensor(0.0149)\n",
      "epoch 0 loss 3.538538787456658e-08 torch.Size([301056, 256]) torch.Size([301056, 256])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 3\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5706e500>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71ea0>\n",
      "totalscore 0.9999997019768045\n",
      "diff sampled tensor(0.0042)\n",
      "epoch 0 loss 5.441925136368327e-08 torch.Size([75264, 512]) torch.Size([75264, 512])\n",
      "diff sampled tensor(0.0042)\n",
      "epoch 0 loss 5.440719185836148e-08 torch.Size([75264, 512]) torch.Size([75264, 512])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 4\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe571093f0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb720e0>\n",
      "totalscore 0.9999995827675505\n",
      "diff sampled tensor(0.0419)\n",
      "epoch 0 loss 1.9486584018782845e-06 torch.Size([18816, 1024]) torch.Size([18816, 1024])\n",
      "diff sampled tensor(0.0419)\n",
      "epoch 0 loss 1.9470233778860483e-06 torch.Size([18816, 1024]) torch.Size([18816, 1024])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 5\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 2 ['1.0', '0.83']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5710b850>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb72770>\n",
      "totalscore 0.9999995231629306\n",
      "epoch 0 loss 0.0 torch.Size([96, 2048]) torch.Size([96, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.5510204081632653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net009\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5710b850>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb711b0>\n",
      "totalscore 0.8262187371532508\n",
      "epoch 0 loss 0.0 torch.Size([96, 2048]) torch.Size([96, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 22.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 1000) (483,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3, 224, 224)\n",
      "accuracy 0.6326530612244898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to _results/1689175585_result_food101_CKA_BS_96_MD_10_T_0.8_TT_0.8_K_2/net010\n",
      "current depth: 1\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb716f0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71120>\n",
      "totalscore 0.9999998807907104\n",
      "diff sampled tensor(0.)\n",
      "epoch 0 loss 1.664057400386723e-05 torch.Size([4816896, 64]) torch.Size([4816896, 64])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 2\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c0ebfd0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb710c0>\n",
      "totalscore 0.9999998211860728\n",
      "diff sampled tensor(25.7547)\n",
      "epoch 0 loss 2.0189481181684832e-05 torch.Size([1204224, 64]) torch.Size([1204224, 64])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 3\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff42557c70>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71060>\n",
      "totalscore 0.9999997019768045\n",
      "diff sampled tensor(70.6109)\n",
      "epoch 0 loss 5.766898641236274e-05 torch.Size([1204224, 128]) torch.Size([1204224, 128])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 4\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7eff42554b50>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb71000>\n",
      "totalscore 0.9999995827675505\n",
      "diff sampled tensor(79.0356)\n",
      "epoch 0 loss 0.0002633728328787367 torch.Size([301056, 128]) torch.Size([301056, 128])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 5\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe3c0ebaf0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70fa0>\n",
      "totalscore 0.9999994635583107\n",
      "diff sampled tensor(159.1844)\n",
      "epoch 0 loss 0.0005354806018571946 torch.Size([301056, 256]) torch.Size([301056, 256])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 6\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe43217a60>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70f40>\n",
      "totalscore 0.9999992847444723\n",
      "diff sampled tensor(231.6651)\n",
      "epoch 0 loss 0.000792250697672063 torch.Size([301056, 256]) torch.Size([301056, 256])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 7\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 1 ['1.0']\n",
      "curr <stitchnet.stitchonnx.utils.Fragment object at 0x7efe571c9db0>\n",
      "nextf <stitchnet.stitchonnx.utils.Fragment object at 0x7efe5cb70ee0>\n",
      "totalscore 0.999999165535268\n",
      "diff sampled tensor(122.2016)\n",
      "epoch 0 loss 1.2429227756237058e+30 torch.Size([75264, 256]) torch.Size([75264, 256])\n",
      "(96, 3, 224, 224)\n",
      "current depth: 8\n",
      "potential next fragments: 2\n",
      "potential next fragments after thresholding of 0.8: 0 []\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from stitchnet.stitchonnx.report import Report, ReportPlants, ReportKNNHFDataset\n",
    "import traceback\n",
    "scoreMapper = ScoreMapper(nets, data_score, scoring_method='CKA')\n",
    "with ReportKNNHFDataset(EVAL_BATCH_SIZE, f'./_results/{RESULT_NAME}.txt', 'a') as report:\n",
    "    # for _ in tqdm(range(50)):\n",
    "    generator = generate_networks(nets, scoreMapper, data_score, \n",
    "                          threshold=THRESOULD, totalThreshold=TOTAL_THRESOULD, \n",
    "                          maxDepth=MAX_DEPTH, sample=False, K=K)\n",
    "    for i,(s,net) in enumerate(generator):\n",
    "        try:\n",
    "            netname = f\"_results/{RESULT_NAME}/net{k:03}\"\n",
    "            report.evaluate(nets, net, netname, s, dataset_val, dataset_train)\n",
    "            net.save(netname)\n",
    "            k += 1\n",
    "        except Exception as e:\n",
    "            print('ERROR', e)\n",
    "            traceback.print_exc()\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca26740c-821d-4501-a5f8-0ca4e10f7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7968d629-5495-42a0-9fad-2dceb6ec98bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in dataset_train:\n",
    "#     print(x)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29248219-866d-480a-a8e8-85332442f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in dl:\n",
    "#     print(x[0].squeeze(1).shape, x[1].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24ea698b-9d65-4751-aaaa-92b03594e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7948b22-19ef-4ec4-a6e9-4e2ff97806d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report.evaluate(nets, net, netname, s, dataset_val, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "247a84f3-1e47-45a4-9c48-08ce3c379850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnxruntime as ort\n",
    "# ort.get_available_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "745ea6e0-ea56-4a2a-a8cd-c142e8636d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in dataset_train:\n",
    "#     print(x)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ab5d2633-6432-443e-b881-de99afb22330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stitchnet.stitchonnx.utils import accuracy_score_net_plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1359c6f1-ace7-4d29-90f1-955daa986281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor([1,2]).numpy().tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f126de0f-ac3c-4e9e-911e-5c315a1cce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,t in dataset_val:\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a392215-d1e5-400e-9722-6043a8075896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77bdcdb0-9b81-415e-9c1d-167a511a14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score_net_plants(net, dataset_val, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5f2ee5b-79e7-4031-8da9-370ba69998df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# knn.fit(np.ones([100,10]), np.ones([100,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "71a77cd2-0240-4b22-b0c5-748156f96db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = knn.predict(np.ones([1,10]))\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f0e38c8-add9-4b3d-8fe1-726c3c2b20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0062065-28d9-4d1d-84c8-e337735b9ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ca02b-b974-4eea-be3d-c7c40ac4148a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e1ce2-30c1-4741-be84-b9de165be464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stitchnet",
   "language": "python",
   "name": "stitchnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
